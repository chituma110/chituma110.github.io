<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Goals determine what you are going to be"><title>Written Memories:Understanding,Deriving and Extending the LSTM | 程起上的博客</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Written Memories:Understanding,Deriving and Extending the LSTM</h1><a id="logo" href="/.">程起上的博客</a><p class="description">Goals determine what you are going to be</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Written Memories:Understanding,Deriving and Extending the LSTM</h1><div class="post-meta">Aug 25, 2016</div><div class="post-content"><h1 id="写在前面的话"><a href="#写在前面的话" class="headerlink" title="写在前面的话"></a>写在前面的话</h1><p><strong>动机：</strong>锻炼一下自己的英文能力，同时加深对LSTM的理解。</p>
<p>   本文是对<a href="http://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html" target="_blank" rel="noopener">Written Memories: Understanding, Deriving and Extending the LSTM</a>的翻译。</p>
<p><strong>有什么错误请指正！</strong></p>
<p>   当我第一次接触 Long Short-Term Memory networks(LSTM)时，我很难理解它的复杂性。我甚至不理解为什么它以这种方式设计就可以产生很好地 效果。后来，我发现LSTM是可以被理解的。尽管它表面看起来很复杂。LSTM实际上是基于一系列十分简单，甚至优美的对于神经网络的一些深刻思考。我真希望这篇博客是我第一次学习循环神经网络。</p>
<p> <strong>在这篇博文中，我们将做如下几件事情：</strong></p>
<ol>
<li><p>我们将定义recurrent neural networks(RNNs)，我们将关注于RNNs的局限性，从而导致了LSTM的发展。</p>
</li>
<li><p>我们将介绍LSTM结构背后的直觉。然后我们以此为依据拓展LSTM。沿着这个思路，我们将派生出GRU。我们将推导出伪LSTM。而这种LSTM将比传统的LSTM的性能更好。</p>
</li>
<li><p>接下来我们将推广这些直觉的知识是怎么在最近的优秀的结构中得到运用的。比如（highway，残差网络，神经图灵机）。</p>
<p>本文是关于理论知识的，并没有实现的内容，至于使用Tensorflow实现RNNs可以参考如下的博客。<br><a href="http://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html" target="_blank" rel="noopener"> Recurrent Neural Networks in Tensorflow I</a>和<a href="http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html" target="_blank" rel="noopener">Recurrent Neural Networks in Tensorflow II</a>。</p>
</li>
</ol>
<h1 id="内容-快速链接："><a href="#内容-快速链接：" class="headerlink" title="内容/快速链接："></a>内容/快速链接：</h1><ul>
<li>递归神经网络</li>
<li>RNNs可以做什么？选择单位时间</li>
<li>主流的RNNs</li>
<li>信息变形，弥散，和爆炸敏感性</li>
<li>针对信息弥散的充要条件</li>
<li>沿着时间轴的后向传播和梯度弥散敏感度</li>
<li>解决梯度弥散和梯度爆炸问题</li>
<li>LSTMs背后的直觉知识：保存记忆</li>
<li>使用选择性的控制和协调</li>
<li>选择的机制：Gates</li>
<li>将Gates粘合在一起派生出了LSTM原型</li>
<li>三个模型：归一化原型，GRU, 以及伪LSTM</li>
<li>推导LSTM</li>
<li>有窥视孔的LSTM</li>
<li>基本的LSTM与伪LSTM的经验性比较</li>
<li>拓展LSTM</li>
</ul>
<h1 id="要求："><a href="#要求：" class="headerlink" title="要求："></a>要求：</h1><h6 id=""><a href="#" class="headerlink" title=" "></a> </h6><p>​    本篇文章假设读者已将熟悉了：</p>
<ol>
<li><p>前向神经网络</p>
</li>
<li><p>反向传播</p>
</li>
<li><p>基本的线性代数</p>
<p>   我们也将回顾一些其他的知识，让我们从RNNs开始吧。</p>
</li>
</ol>
<h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><p>​    </p>
<p>​    从一个时刻到下一个时刻，我们的大脑就像一个函数一样在工作，它从我们的感官（外部）和我们的思维（内部）得到输入，然后处理一番，得到以动作形式（外部）和新的想法（内部）的输出。例如，我们看到 一只熊，然后我们就会想到单词“熊”，我们可以构建一个神经网络来模拟这种行为，我们可以训练神经网络去联想到单词“熊”当它看到一张熊的照片的时候。但是我们的大脑不仅仅是一个只有一次的函数，它随着时间重复运行。我们看见一只熊，然后我们想到了单词“熊”,然后我们就会想到“跑”， 重要的是，将熊的图片转化为单词“熊的网络和将单词“熊”转化为”跑“的网络是十分相似的。同时，它也是一个 循环的函数，对于这样的函数我们可以用递归神经网络来模拟。</p>
<p>​    RNN网络是一系列完全相同的前向神经网络的组合，每一个前向神经网络处理每一个时间步，我们都称之为“RNN cells”.值得注意的是这个定义比我们通常的对RNN的定义(“vanilla” RNN将在下文中介绍作为LSTM的先驱)更为广泛。这些cells可以处理他们自己的输出，允许他们(cells)被组合。他们同时可以处理外部的输入，产生外部的输出。下面是单独一个RNN cell 的示例图：</p>
<p><img src="/2016/08/25/Written-LSTM/NH_SingleRNNcell.png" alt="Single_RNN_cell"></p>
<p>下面是三个RNN cell的组合的示例图：</p>
<p><img src="/2016/08/25/Written-LSTM/NH_ComposedRNNcells.png" alt="NH_Composed_RNN_cells"></p>
<p>​    你可以认为这种循环的输出是“状态”，他们被输入到下一个时间步。所以，一个“RNN cell” 接收前一个状态和现有的输入(可选)，然后输出现在的状态，以及现在的输出(可选)。</p>
<p>下面是“RNN cell” 的代数描述:<br>$$<br>\binom{s<em>{t}}{o</em>{t}} = f\binom{s<em>{t-1}}{x</em>{t}}<br>$$<br>其中：<br>$$<br>s<em>{t}和s</em>{t-1}表示当前和前一刻状态<br>$$</p>
<p>$$<br>o_{t}表示当前时刻的输出(可能是空)<br>$$</p>
<p>$$<br>x_{t}是当前状态的输入,f是当前的循环函数<br>$$</p>
<p>​    我们就大脑的运行方式：当前的神经元活动将会替代之前的神经元活动。我们可以将RNNs看做这种原地的操作，因为”RNN cell”是完全相同的，他们可以被看做是相同的对象，只是在每一个时间步他们的状态会被覆盖。下面是这个框架的示例图：</p>
<p><img src="/2016/08/25/Written-LSTM/NH_StateLoop.png" alt="NH_State_Loop"></p>
<p>​    大多数对于RNNs的介绍都是从”single cell loop”框架开始的，但是我认为读者应该会认为以上连续的框架会更加的直观，特别是考虑到反向传播的时候。当我们从”single cell loop”开始学习的额时候。RNN会被引入”unrolled”以得到上面所提到的连续的框架。</p>
<h1 id="RNNs可以做什么？选择单位时间"><a href="#RNNs可以做什么？选择单位时间" class="headerlink" title="RNNs可以做什么？选择单位时间"></a>RNNs可以做什么？选择单位时间</h1><p>​    </p>
</div><div class="tags"><a href="/tag/LSTM/">LSTM</a></div><div class="post-nav"><a class="pre" href="/2017/03/15/python-parse-VOCxml-and-show/">python_parse_VOCxml_and_show</a><a class="next" href="/2016/08/22/CUDA_installration/">CUDA安装教程</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://yoursite.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tag/python-PIL-matplotlib/" style="font-size: 15px;">python, PIL, matplotlib</a> <a href="/tag/卷积/" style="font-size: 15px;">卷积</a> <a href="/tag/python-Algorithms/" style="font-size: 15px;">python, Algorithms</a> <a href="/tag/python-object-detection/" style="font-size: 15px;">python, object_detection</a> <a href="/tag/LSTM/" style="font-size: 15px;">LSTM</a> <a href="/tag/python-caffe/" style="font-size: 15px;">python, caffe</a> <a href="/tag/opencv-libatk/" style="font-size: 15px;">opencv, libatk</a> <a href="/tag/CUDA/" style="font-size: 15px;">CUDA</a> <a href="/tag/Python/" style="font-size: 15px;">Python</a> <a href="/tag/python-random/" style="font-size: 15px;">python,  random</a> <a href="/tag/python-exe/" style="font-size: 15px;">python exe</a> <a href="/tag/Python-LMDB/" style="font-size: 15px;">Python, LMDB</a> <a href="/tag/test/" style="font-size: 15px;">test</a> <a href="/tag/python-VOCxml/" style="font-size: 15px;">python VOCxml</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/05/17/python-random/">Python-Random</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/17/hello-world/">Hello World</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/06/caffe_install/">Caffe安装新坑</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/05/07/python-file-print/">Python_file_print</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/28/caffe-solver小结/">Caffe_solver小结</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/28/python之于LMDB初探/">python之于LMDB初探</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/22/Python写caffe层/">Python写caffe层</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/21/test/">Test</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/21/python-to-exe/">Python创建exe文件</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/19/Object_detection_NMS/">Object_detection_NMS</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">程起上的博客.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>